# stage4_sft_lora_prop.py
"""
Stage-4â€ƒSFT for molecular property-prediction (HIV / â€¦)
-------------------------------------------------------
â€¢ è®­ç»ƒé›†ï¼šå…ˆå–åŸå§‹æ•°æ® â†’ 10 % åˆ’ä½œéªŒè¯é›†(ä¸åšä»»ä½•å‡è¡¡)  
            â†’ å‰©ä½™æ ·æœ¬éšæœºé‡‡æ ·æœ€å¤š 1 ä¸‡æ¡ â†’ å¯¹è¯¥ 1 ä¸‡æ¡åš minority-oversampling  
â€¢ è¯„ä¼°æŒ‡æ ‡ï¼šAcc / P / R / F1ï¼Œä¸”æ‰“å°éªŒè¯é›†ä¸­ç¬¬ä¸€æ¡æ ·æœ¬ Prompt / Gen / Ref  
â€¢ å…¶ä½™ä¿æŒä¸ä¸Šä¸€ç‰ˆä¸€è‡´
"""
from __future__ import annotations
import os
import sys
import json
import math
import random
import argparse
import functools
import logging
import re
import shutil
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import AutoTokenizer, LlamaForCausalLM, get_linear_schedule_with_warmup
from peft import LoraConfig, get_peft_model

from atomdisc.utils.gnn_vq_utils import set_seed, get_logger, load_gnn_vq_models
from atomdisc.tokenization.mol_tokenizer import convert_text_smiles_to_mol_tokens

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dataset & collate
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class MolVQPropertyDataset(Dataset):
    def __init__(self, recs: List[Dict[str, Any]], tok, gnn, vq, dev,
                 max_prompt_len=1024, max_resp_len=16):
        self.recs = recs
        self.tok  = tok
        self.gnn, self.vq, self.dev = gnn, vq, dev
        self.max_prompt_len, self.max_resp_len = max_prompt_len, max_resp_len
        self.gnn.eval(); self.vq.eval()
        if self.tok.pad_token_id is None:
            self.tok.pad_token = self.tok.eos_token
            self.tok.pad_token_id = self.tok.eos_token_id

    @torch.no_grad()
    def _smi2tok(self, smi: str) -> str:
        return convert_text_smiles_to_mol_tokens(f"<smiles>{smi}</smiles>",
                                                 self.gnn, self.vq, self.dev) or ""

    def __len__(self): return len(self.recs)

    def _extract_subtask(self, instruction: str, rec: Dict[str, Any]) -> str:
        """ä»æŒ‡ä»¤æˆ–è®°å½•ä¸­æå–å­ä»»åŠ¡æ ‡è¯†"""
        # ä¼˜å…ˆä½¿ç”¨è®°å½•ä¸­çš„ subtask/task å­—æ®µ
        if "subtask" in rec:
            return str(rec["subtask"])
        if "task" in rec:
            return str(rec["task"])
        
        # ç›´æ¥ä½¿ç”¨å®Œæ•´çš„instructionä½œä¸ºå­ä»»åŠ¡æ ‡è¯†
        # å› ä¸ºç›¸åŒä»»åŠ¡çš„instructionç›¸åŒï¼Œä¸åŒä»»åŠ¡çš„instructionä¸åŒ
        return instruction.strip()

    def __getitem__(self, idx: int):
        r = self.recs[idx]
        smi, label, instr = r["input"], str(r["output"]).strip(), r["instruction"]
        subtask = self._extract_subtask(instr, r)
        
        prompt = (
            f"### Instruction:\n{instr}\n\n"
            f"### Input:\nMolecule (SMILES):\n{smi}\n"
            f"Molecule (Structure):\n{self._smi2tok(smi)}\n\n"
            f"### Response:"
        )
        pt = self.tok(prompt, truncation=True, max_length=self.max_prompt_len, add_special_tokens=False)
        rt = self.tok(label , truncation=True, max_length=self.max_resp_len , add_special_tokens=False)
        ids  = [self.tok.bos_token_id] + pt["input_ids"] + rt["input_ids"] + [self.tok.eos_token_id]
        lbls = [-100]*(1+len(pt["input_ids"])) + rt["input_ids"] + [self.tok.eos_token_id]
        ids_t  = torch.tensor(ids ,dtype=torch.long)
        lbls_t = torch.tensor(lbls,dtype=torch.long)
        return {"input_ids":ids_t,"attention_mask":torch.ones_like(ids_t),
                "labels":lbls_t,"prompt":prompt,"target_text":label,"subtask":subtask}

def collate_fn(batch, pad_id:int):
    ids  = pad_sequence([b["input_ids"] for b in batch], batch_first=True, padding_value=pad_id)
    labs = pad_sequence([b["labels"]    for b in batch], batch_first=True, padding_value=-100)
    msk  = ids.ne(pad_id).long()
    return {"input_ids":ids,"attention_mask":msk,"labels":labs,
            "prompt":[b["prompt"] for b in batch],
            "target_text":[b["target_text"] for b in batch],
            "subtask":[b["subtask"] for b in batch]}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Scaffold split utility
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def get_scaffold(smiles: str) -> str:
    """ä»SMILESè·å–Murcko scaffold"""
    if not RDKIT_AVAILABLE:
        return smiles  # é™çº§åˆ°SMILESæœ¬èº«
    
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return smiles  # è§£æå¤±è´¥ï¼Œè¿”å›åŸSMILES
        
        scaffold = MurckoScaffold.GetScaffoldForMol(mol)
        if scaffold is None:
            return smiles  # scaffoldæå–å¤±è´¥
            
        scaffold_smiles = Chem.MolToSmiles(scaffold)
        if not scaffold_smiles:  # ç©ºå­—ç¬¦ä¸²
            return smiles
            
        return scaffold_smiles
    except Exception as e:
        # å¦‚æœå‡ºç°ä»»ä½•å¼‚å¸¸ï¼Œè¿”å›åŸSMILES
        return smiles

def scaffold_split(recs: List[Dict[str, Any]], val_ratio: float = 0.1, test_ratio: float = 0.1,
                  seed: int = 42, balanced: bool = True) -> tuple:
    """
    åŸºäºscaffoldè¿›è¡Œæ•°æ®åˆ†å‰²
    Args:
        recs: æ•°æ®è®°å½•åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å½•åŒ…å« 'input' (SMILES) å­—æ®µ
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        seed: éšæœºç§å­
        balanced: True=éšæœºscaffold splitï¼ˆæ›´å¹³è¡¡ï¼‰; False=ç¡®å®šæ€§splitï¼ˆå¯èƒ½ä¸å¹³è¡¡ï¼‰
    Returns:
        (train_recs, val_recs, test_recs)
    """
    total = len(recs)
    if total == 0:
        return [], [], []

    if not RDKIT_AVAILABLE:
        # é™çº§åˆ°éšæœºåˆ†å‰²
        random.seed(seed)
        random.shuffle(recs)
        n_val = max(1, int(total * val_ratio))
        n_test = max(1, int(total * test_ratio))
        n_val = min(n_val, total - 2) if total >= 3 else n_val
        n_test = min(n_test, total - n_val - 1) if total - n_val >= 2 else n_test
        val_recs = recs[:n_val]
        test_recs = recs[n_val:n_val + n_test]
        train_recs = recs[n_val + n_test:]
        return train_recs, val_recs, test_recs

    # æŒ‰scaffoldåˆ†ç»„
    scaffold_to_indices = defaultdict(list)
    for i, rec in enumerate(recs):
        smiles = rec["input"]
        scaffold = get_scaffold(smiles)
        scaffold_to_indices[scaffold].append(i)

    scaffold_groups = list(scaffold_to_indices.values())

    if balanced:
        random.seed(seed)
        random.shuffle(scaffold_groups)
    else:
        scaffold_groups = sorted(scaffold_groups, key=len, reverse=True)

    target_val = total * val_ratio
    target_test = total * test_ratio
    val_indices, test_indices, train_indices = [], [], []

    for group in scaffold_groups:
        if len(val_indices) + len(group) <= target_val:
            val_indices.extend(group)
        elif len(test_indices) + len(group) <= target_test:
            test_indices.extend(group)
        else:
            train_indices.extend(group)

    # è‹¥è®­ç»ƒé›†ä¸ºç©ºæˆ–æ¯”ä¾‹ä¸è¶³ï¼Œè¡¥å›å‰©ä½™æ•°æ®
    remaining = set(range(total)) - set(val_indices) - set(test_indices) - set(train_indices)
    train_indices.extend(list(remaining))

    train_recs = [recs[i] for i in train_indices]
    val_recs = [recs[i] for i in val_indices]
    test_recs = [recs[i] for i in test_indices]

    return train_recs, val_recs, test_recs

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Metrics & evaluation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _prf(preds, refs):
    tp=sum(p==r=="yes" for p,r in zip(preds,refs))
    fp=sum(p=="yes" and r=="no"  for p,r in zip(preds,refs))
    fn=sum(p=="no"  and r=="yes" for p,r in zip(preds,refs))
    prec=tp/(tp+fp+1e-8); rec=tp/(tp+fn+1e-8); f1=2*prec*rec/(prec+rec+1e-8)
    acc=sum(p==r for p,r in zip(preds,refs)) / (len(refs) or 1)
    return acc,prec,rec,f1

def extract_auroc_from_log(log_file):
    """æå–æ‰€æœ‰ Macro-AUCï¼Œè¿”å›æœ€å¤§å€¼ï¼ˆä¼˜å…ˆï¼‰ï¼Œè‹¥æ— åˆ™æå– ROC-AUCï¼ˆå…¼å®¹è€ç‰ˆæœ¬ï¼‰"""
    macro_aucs = []
    micro_aucs = []
    with open(log_file) as f:
        for line in f:
            # ä¼˜å…ˆæå– Macro-AUCï¼ˆæ–°ç‰ˆæœ¬ï¼‰
            m_macro = re.search(r"Macro-AUC=([0-9\.]+)", line)
            if m_macro:
                macro_aucs.append(float(m_macro.group(1)))
            
            # å…¼å®¹è€ç‰ˆæœ¬çš„ ROC-AUCï¼ˆå³ Micro-AUCï¼‰
            m_micro = re.search(r"ROC-AUC=([0-9\.]+)", line)
            if m_micro:
                micro_aucs.append(float(m_micro.group(1)))
    
    # ä¼˜å…ˆè¿”å› Macro-AUCï¼Œè‹¥æ— åˆ™è¿”å› Micro-AUC
    if macro_aucs:
        return max(macro_aucs)
    elif micro_aucs:
        return max(micro_aucs)
    else:
        return float("nan")

def evaluate(epoch, model, tok, loader, dev, logger, args):
    if loader is None:
        return
    model.eval()
    preds_all, refs_all, scores_all, subtasks_all = [], [], [], []
    first = False

    # è·å– yes/no çš„ token IDï¼Œå–ç¬¬ä¸€ä¸ª token å°±è¡Œï¼ˆæ›´ç¨³ï¼‰
    tok_yes = tok.encode("yes", add_special_tokens=False)
    tok_no  = tok.encode("no",  add_special_tokens=False)
    id_yes = tok_yes[0]
    id_no  = tok_no[0]

    # å¯é€‰ï¼šæ‰“å°ä¸€ä¸‹å®é™…ç”¨çš„ tokenï¼ˆè°ƒè¯•ç”¨ï¼‰
    logger.info(f"[Token] yes â†’ {tok.convert_ids_to_tokens([id_yes])[0]} (id={id_yes})")
    logger.info(f"[Token] no  â†’ {tok.convert_ids_to_tokens([id_no])[0]} (id={id_no})")

    with torch.no_grad():
        for batch in tqdm(loader, desc=f"Eval@{epoch}", unit="batch"):
            for prm, ref, subtask in zip(batch["prompt"], batch["target_text"], batch["subtask"]):
                inp = tok(prm, return_tensors="pt", max_length=args.max_seq_length, truncation=True).to(dev)

                # è·å–æ¨¡å‹è¾“å‡ºçš„ logitsï¼Œå–æœ€åä¸€ä¸ª token çš„ logits
                out = model(**inp)
                logits = out.logits[0, -1]  # shape: [vocab_size]

                # è·å– "yes"/"no" æ¦‚ç‡
                prob = torch.softmax(logits[[id_yes, id_no]], dim=-1)
                p_yes = prob[0].item()
                scores_all.append(p_yes)

                # ä½¿ç”¨0.5ä¸ºé˜ˆå€¼åšåˆ†ç±»
                pred = "yes" if p_yes >= 0.5 else "no"
                preds_all.append(pred)
                refs_all.append(ref.lower())
                subtasks_all.append(subtask)

                if not first:
                    print("\n=== Eval Sample 0 ===")
                    print("Prompt:\n", prm)
                    print("Reference:", ref)
                    print("P(yes):", p_yes)
                    print("Predicted:", pred)
                    print("Subtask:", subtask)
                    print("=====================\n")
                    first = True

    # æŒ‡æ ‡è®¡ç®—ï¼ˆæ•´ä½“ï¼‰
    acc, p, r, f1 = _prf(preds_all, refs_all)

    # microï¼ˆpooledï¼‰AUCï¼ˆä»…ä½œå‚è€ƒï¼‰
    y_all = np.array([1 if r == "yes" else 0 for r in refs_all])
    try:
        micro_auc = roc_auc_score(y_all, np.array(scores_all))
    except ValueError:
        micro_auc = float("nan")

    # macroï¼ˆæŒ‰å­ä»»åŠ¡ï¼‰AUC â€”â€” ç¤¾åŒºå£å¾„
    task2y, task2s = defaultdict(list), defaultdict(list)
    for y, s, t in zip(y_all, scores_all, subtasks_all):
        task2y[t].append(y)
        task2s[t].append(s)

    aucs = []
    skipped = 0
    task_details = []
    
    for t in sorted(task2y.keys()):
        y = np.array(task2y[t])
        s = np.array(task2s[t])
        n_pos = np.sum(y == 1)
        n_neg = np.sum(y == 0)
        
        if len(np.unique(y)) < 2:  # æ— æ­£æˆ–æ— è´Ÿï¼šè·³è¿‡
            skipped += 1
            task_details.append(f"{t}: SKIPPED (pos={n_pos}, neg={n_neg})")
            continue
        
        try:
            task_auc = roc_auc_score(y, s)
            aucs.append(task_auc)
            task_details.append(f"{t}: AUC={task_auc:.4f} (pos={n_pos}, neg={n_neg})")
        except ValueError:
            skipped += 1
            task_details.append(f"{t}: ERROR (pos={n_pos}, neg={n_neg})")
    
    macro_auc = float(np.mean(aucs)) if aucs else float("nan")

    # è¯¦ç»†æ—¥å¿—è¾“å‡º
    logger.info(f"[Eval] Overall: Acc={acc:.4f}  P={p:.4f}  R={r:.4f}  F1={f1:.4f}")
    logger.info(f"[Eval] Macro-AUC={macro_auc:.4f} over {len(aucs)} tasks (skipped={skipped}); Micro-AUC={micro_auc:.4f}")
    
    # æ‰“å°æ¯ä¸ªå­ä»»åŠ¡çš„è¯¦æƒ…
    logger.info("[Task Details]")
    for detail in task_details:
        logger.info(f"  {detail}")
    
    model.train()

def evaluate_single_task(epoch, model, tok, loader, dev, logger, args):
    """è¯„ä¼°å•ä¸ªä»»åŠ¡ï¼Œè¿”å›AUC"""
    if loader is None:
        return float("nan")
    
    model.eval()
    preds_all, refs_all, scores_all = [], [], []
    first = False

    # è·å– yes/no çš„ token ID
    tok_yes = tok.encode("yes", add_special_tokens=False)
    tok_no  = tok.encode("no",  add_special_tokens=False)
    id_yes = tok_yes[0]
    id_no  = tok_no[0]

    with torch.no_grad():
        for batch in tqdm(loader, desc=f"Eval@{epoch}", unit="batch"):
            for prm, ref, subtask in zip(batch["prompt"], batch["target_text"], batch["subtask"]):
                inp = tok(prm, return_tensors="pt", max_length=args.max_seq_length, truncation=True).to(dev)

                out = model(**inp)
                logits = out.logits[0, -1]

                prob = torch.softmax(logits[[id_yes, id_no]], dim=-1)
                p_yes = prob[0].item()
                scores_all.append(p_yes)

                pred = "yes" if p_yes >= 0.5 else "no"
                preds_all.append(pred)
                refs_all.append(ref.lower())

                if not first:
                    logger.info(f"\n=== Eval Sample 0 ===")
                    logger.info(f"Prompt:\n{prm}")
                    logger.info(f"Reference: {ref}")
                    logger.info(f"P(yes): {p_yes}")
                    logger.info(f"Predicted: {pred}")
                    logger.info("=====================\n")
                    first = True

    # è®¡ç®—æŒ‡æ ‡
    acc, p, r, f1 = _prf(preds_all, refs_all)

    # è®¡ç®—AUC
    y_all = np.array([1 if r == "yes" else 0 for r in refs_all])
    try:
        task_auc = roc_auc_score(y_all, np.array(scores_all))
    except ValueError:
        task_auc = float("nan")

    logger.info(f"[Eval@{epoch}] Acc={acc:.4f}  P={p:.4f}  R={r:.4f}  F1={f1:.4f}  AUC={task_auc:.4f}")
    
    model.train()
    return task_auc

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Training
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def sft_lora_single_task(args, task_name, task_train_recs, task_val_recs, gnn, vq, tok, base, oversample=True, version_suffix=""):
    """è®­ç»ƒå•ä¸ªä»»åŠ¡çš„LoRAæ¨¡å‹"""
    dev=torch.device(args.device if torch.cuda.is_available() else "cpu")
    
    # ä¸ºå½“å‰ä»»åŠ¡åˆ›å»ºä¸“ç”¨è¾“å‡ºç›®å½•
    task_dir_name = f"task_{task_name[:30].replace('?', '').replace(' ', '_')}{version_suffix}"
    task_output_dir = Path(args.output_dir) / task_dir_name
    task_output_dir.mkdir(parents=True, exist_ok=True)
    
    logger=get_logger(f"SFT_{task_name[:20]}{version_suffix}", task_output_dir/"train.log")
    logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_name} (oversample={oversample})")
    logger.info(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(task_train_recs)}, éªŒè¯æ ·æœ¬: {len(task_val_recs)}")
    
    # ç»Ÿè®¡å½“å‰ä»»åŠ¡çš„æ ‡ç­¾åˆ†å¸ƒ
    train_yes = sum(1 for r in task_train_recs if str(r["output"]).strip().lower() == "yes")
    train_no = len(task_train_recs) - train_yes
    val_yes = sum(1 for r in task_val_recs if str(r["output"]).strip().lower() == "yes")
    val_no = len(task_val_recs) - val_yes
    
    logger.info(f"ğŸ“ˆ è®­ç»ƒé›†åˆ†å¸ƒ: {train_yes} yes, {train_no} no ({train_yes/(train_yes+train_no):.1%} positive)")
    logger.info(f"ğŸ“‰ éªŒè¯é›†åˆ†å¸ƒ: {val_yes} yes, {val_no} no ({val_yes/(val_yes+val_no):.1%} positive)")

    # --- fresh LoRA for this task ---
    lcfg=LoraConfig(r=args.lora_r,lora_alpha=args.lora_alpha,lora_dropout=args.lora_dropout,
                    target_modules=[m.strip() for m in args.lora_target_modules.split(',')],
                    bias="none",task_type="CAUSAL_LM")
    
    # âœ… ä¸ºæ¯ä¸ªä»»åŠ¡é‡æ–°åŠ è½½base modelï¼ˆé¿å…LoRAå†²çªï¼‰
    logger.info("ğŸ“‹ ä¸ºå½“å‰ä»»åŠ¡åŠ è½½ç‹¬ç«‹çš„base model")
    if Path(args.stage2_model_path).is_dir():
        fresh_base=LlamaForCausalLM.from_pretrained(args.stage2_model_path, torch_dtype=torch.bfloat16).to(dev)
    else:
        fresh_base=LlamaForCausalLM.from_pretrained(args.base_llm_model_path, torch_dtype=torch.bfloat16).to(dev)
        if hasattr(base, 'config') and base.config.vocab_size != len(tok):
            fresh_base.resize_token_embeddings(len(tok))
    
    model=get_peft_model(fresh_base,lcfg).train()

    # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    train_recs = task_train_recs
    val_recs = task_val_recs
    
    # å•ä»»åŠ¡æ•°æ®è¿‡é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if oversample and train_recs:
        yes=[r for r in train_recs if str(r["output"]).strip().lower()=="yes"]
        no =[r for r in train_recs if str(r["output"]).strip().lower()=="no"]
        if yes and no:
            if len(yes)<len(no):
                yes= yes * math.ceil(len(no)/len(yes))
            elif len(no)<len(yes):
                no = no  * math.ceil(len(yes)/len(no))
        train_recs = (yes+no) if yes and no else train_recs
        random.shuffle(train_recs)
        logger.info(f"ğŸ“Š ä»»åŠ¡å†…å¹³è¡¡: {len(train_recs)} æ ·æœ¬ (yes {sum(str(r['output']).lower()=='yes' for r in train_recs)}, "
                    f"no {sum(str(r['output']).lower()=='no' for r in train_recs)})") 
    else:
        logger.info(f"ğŸ“Š ä¸è¿›è¡Œè¿‡é‡‡æ ·: {len(train_recs)} æ ·æœ¬")

    # --- dataloaders ---
    train_ds=MolVQPropertyDataset(train_recs,tok,gnn,vq,dev,args.max_prompt_len,args.max_response_len)
    val_ds  =MolVQPropertyDataset(val_recs ,tok,gnn,vq,dev,args.max_prompt_len,args.max_response_len)
    collate=functools.partial(collate_fn,pad_id=tok.pad_token_id)
    train_lo=DataLoader(train_ds,batch_size=args.batch_size,shuffle=True,
                        num_workers=args.num_workers,collate_fn=collate,drop_last=True)
    val_lo  =DataLoader(val_ds ,batch_size=args.eval_batch_size,shuffle=False,
                        num_workers=args.num_workers,collate_fn=collate)

    # --- optimiser ---
    params=[p for p in model.parameters() if p.requires_grad]
    opt=torch.optim.AdamW(params,lr=args.learning_rate,weight_decay=args.weight_decay)
    steps_ep=math.ceil(len(train_lo)/args.gradient_accumulation_steps)
    tot_steps=steps_ep*args.num_epochs
    sch=get_linear_schedule_with_warmup(opt,int(args.warmup_ratio*tot_steps),tot_steps)

    # --- training ---
    best_auc = float('-inf')  # è®°å½•æœ€ä½³AUC
    all_aucs = []  # è®°å½•æ‰€æœ‰epochçš„AUC
    
    for ep in range(args.num_epochs):
        model.train(); ep_loss=0; opt.zero_grad()
        pbar=tqdm(train_lo,desc=f"Ep {ep+1}/{args.num_epochs}")
        for i,b in enumerate(pbar):
            ids,msk,lbl=b["input_ids"].to(dev),b["attention_mask"].to(dev),b["labels"].to(dev)
            loss=model(input_ids=ids,attention_mask=msk,labels=lbl).loss
            (loss/args.gradient_accumulation_steps).backward(); ep_loss+=loss.item()
            if (i+1)%args.gradient_accumulation_steps==0:
                torch.nn.utils.clip_grad_norm_(params,args.max_grad_norm)
                opt.step(); sch.step(); opt.zero_grad()
            pbar.set_postfix({"CE":f"{loss.item():.4f}"})
        logger.info(f"Ep {ep+1} mean CE = {ep_loss/len(train_lo):.4f}")

        if (ep+1)%args.eval_every_epochs==0:
            task_auc = evaluate_single_task(ep+1,model,tok,val_lo,dev,logger,args)
            all_aucs.append(task_auc)
            
            # æ›´æ–°æœ€ä½³AUC
            if not math.isnan(task_auc) and task_auc > best_auc:
                best_auc = task_auc
                logger.info(f"ğŸ¯ æ–°çš„æœ€ä½³AUC: {best_auc:.4f} (Epoch {ep+1})")
            
    # æ±‡æ€»AUCä¿¡æ¯
    valid_aucs = [auc for auc in all_aucs if not math.isnan(auc)]
    if valid_aucs:
        max_auc = max(valid_aucs)
        final_auc = all_aucs[-1] if all_aucs else float("nan")  # æœ€åä¸€ä¸ªepochçš„AUC
        
        logger.info(f"ğŸ“Š AUCæ±‡æ€»: æœ€é«˜={max_auc:.4f}, æœ€ç»ˆ={final_auc:.4f}, æ‰€æœ‰epoch={[f'{v:.4f}' for v in valid_aucs]}")
        result_auc = max_auc
    else:
        logger.warning("âš ï¸ æ‰€æœ‰epochçš„AUCéƒ½æ˜¯NaN")
        result_auc = float("nan")
    
    # ğŸš¨ å…³é”®ï¼šæ¸…ç†æ˜¾å­˜
    logger.info("ğŸ§¹ æ¸…ç†ä»»åŠ¡æ˜¾å­˜...")
    del model, fresh_base, train_ds, val_ds, train_lo, val_lo, opt, sch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # æ˜¾ç¤ºæ¸…ç†åçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        memory_allocated = torch.cuda.memory_allocated(dev) / 1024**3  # GB
        memory_reserved = torch.cuda.memory_reserved(dev) / 1024**3   # GB
        logger.info(f"ğŸ’¾ æ˜¾å­˜æ¸…ç†å: å·²åˆ†é…={memory_allocated:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB")
    
    return result_auc, task_output_dir

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def sft_lora_all_tasks(args):
    """ä¸»æ§å‡½æ•°ï¼šç‹¬ç«‹è®­ç»ƒæ‰€æœ‰å­ä»»åŠ¡"""
    set_seed(args.seed)
    dev = torch.device(args.device if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºæ€»è¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main_logger = get_logger("SFT_AllTasks", Path(args.output_dir)/"main.log")
    main_logger.info(f"ğŸš€ å¼€å§‹ç‹¬ç«‹è®­ç»ƒæ‰€æœ‰å­ä»»åŠ¡...")
    main_logger.info(f"Device={dev}")

    # --- åŠ è½½å…±äº«ç»„ä»¶ ---
    main_logger.info("ğŸ“š åŠ è½½å…±äº«ç»„ä»¶...")
    
    # frozen GNN+VQ
    gnn, vq = load_gnn_vq_models(args.stage1_gnn_vq_checkpoint_path, dev)
    
    # tokenizer & base
    if Path(args.stage2_model_path).is_dir():
        tok = AutoTokenizer.from_pretrained(args.stage2_model_path, use_fast=True)
        base = LlamaForCausalLM.from_pretrained(args.stage2_model_path, torch_dtype=torch.bfloat16).to(dev)
    else:
        tok = AutoTokenizer.from_pretrained(args.base_tokenizer_path, use_fast=True)
        base = LlamaForCausalLM.from_pretrained(args.base_llm_model_path, torch_dtype=torch.bfloat16).to(dev)
        extra = ["<mol>","</mol>"] + [f"<atom_{i}>" for i in range(args.vq_codebook_size)]
        tok.add_special_tokens({"additional_special_tokens": extra})
        base.resize_token_embeddings(len(tok))
    
    if tok.pad_token_id is None:
        tok.pad_token = tok.eos_token
        tok.pad_token_id = tok.eos_token_id

    # --- åŠ è½½å’Œåˆ†å‰²æ•°æ® ---
    main_logger.info("ğŸ“Š åŠ è½½åŸå§‹æ•°æ®...")
    all_recs = []
    for t in [x.strip() for x in args.tasks.split(",") if x.strip()]:
        fp = Path(args.dataset_root) / f"{t}.json"
        if fp.is_file():
            all_recs += json.loads(fp.read_text())
    random.shuffle(all_recs)
    main_logger.info(f"æ€»æ•°æ®é‡: {len(all_recs)}")

    # Scaffoldåˆ†å‰²
    if RDKIT_AVAILABLE and args.use_scaffold_split == 1:
        main_logger.info("ğŸ§¬ ä½¿ç”¨ Scaffold Split è¿›è¡Œæ•°æ®åˆ†å‰²...")
        train_recs, val_recs, test_recs = scaffold_split(all_recs, val_ratio=0.1, test_ratio=0.1, seed=args.seed, balanced=args.balanced_scaffold)
        main_logger.info("âœ… Scaffold Split å®Œæˆ")
    else:
        main_logger.info("ğŸ“ ä½¿ç”¨éšæœºåˆ†å‰²...")
        n_val = max(1, int(0.1 * len(all_recs)))
        n_test = max(1, int(0.1 * len(all_recs)))
        n_val = min(n_val, len(all_recs) - 2) if len(all_recs) >= 3 else n_val
        n_test = min(n_test, len(all_recs) - n_val - 1) if len(all_recs) - n_val >= 2 else n_test
        val_recs = all_recs[:n_val]
        test_recs = all_recs[n_val:n_val + n_test]
        train_recs = all_recs[n_val + n_test:]

    # --- æŒ‰å­ä»»åŠ¡åˆ†ç»„ ---
    main_logger.info("ğŸ¯ æŒ‰å­ä»»åŠ¡åˆ†ç»„æ•°æ®...")
    
    def group_by_subtask(recs):
        task_groups = defaultdict(list)
        for r in recs:
            subtask = r.get("subtask") or r.get("task") or r["instruction"].strip()
            task_groups[subtask].append(r)
        return task_groups
    
    train_task_groups = group_by_subtask(train_recs)
    val_task_groups = group_by_subtask(val_recs)
    test_task_groups = group_by_subtask(test_recs)
    
    # ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰ç›¸åŒçš„ä»»åŠ¡
    common_tasks = set(train_task_groups.keys()) & set(val_task_groups.keys()) & set(test_task_groups.keys())
    main_logger.info(f"å‘ç° {len(common_tasks)} ä¸ªå…±åŒå­ä»»åŠ¡")
    
    if len(common_tasks) != len(train_task_groups) or len(common_tasks) != len(val_task_groups) or len(common_tasks) != len(test_task_groups):
        main_logger.warning("âš ï¸ è®­ç»ƒé›†ã€éªŒè¯é›†æˆ–æµ‹è¯•é›†çš„ä»»åŠ¡ä¸å®Œå…¨ä¸€è‡´ï¼")
    
    # --- é€ä¸ªè®­ç»ƒå­ä»»åŠ¡ ---
    task_results = {}
    failed_tasks = []
    
    for i, task_name in enumerate(sorted(common_tasks), 1):
        main_logger.info(f"\nğŸ¯ [{i}/{len(common_tasks)}] å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_name[:50]}...")
        
        # æ˜¾ç¤ºå½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(dev) / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved(dev) / 1024**3   # GB
            main_logger.info(f"ğŸ’¾ ä»»åŠ¡å¼€å§‹å‰æ˜¾å­˜: å·²åˆ†é…={memory_allocated:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB")
        
        task_train_recs = train_task_groups[task_name]
        task_val_recs = val_task_groups[task_name]
        task_test_recs = test_task_groups[task_name]
        
        # æ£€æŸ¥æ•°æ®é‡
        train_yes = sum(1 for r in task_train_recs if str(r["output"]).strip().lower() == "yes")
        train_no = len(task_train_recs) - train_yes
        val_yes = sum(1 for r in task_val_recs if str(r["output"]).strip().lower() == "yes")
        val_no = len(task_val_recs) - val_yes
        test_yes = sum(1 for r in task_test_recs if str(r["output"]).strip().lower() == "yes")
        test_no = len(task_test_recs) - test_yes
        
        main_logger.info(f"è®­ç»ƒ: {train_yes}+{train_no}={len(task_train_recs)}, éªŒè¯: {val_yes}+{val_no}={len(task_val_recs)}, æµ‹è¯•: {test_yes}+{test_no}={len(task_test_recs)}")
        
        # è·³è¿‡æ•°æ®å¤ªå°‘æˆ–å•ä¸€ç±»åˆ«çš„ä»»åŠ¡
        if len(task_train_recs) < 10 or len(task_val_recs) < 3 or len(task_test_recs) < 3:
            main_logger.warning(f"âš ï¸ è·³è¿‡ä»»åŠ¡ {task_name[:30]}... (æ•°æ®å¤ªå°‘)")
            failed_tasks.append(task_name)
            continue
            
        if min(train_yes, train_no) == 0 or min(val_yes, val_no) == 0 or min(test_yes, test_no) == 0:
            main_logger.warning(f"âš ï¸ è·³è¿‡ä»»åŠ¡ {task_name[:30]}... (å•ä¸€ç±»åˆ«)")
            failed_tasks.append(task_name)
            continue
        
        try:
            # è®­ç»ƒå½“å‰ä»»åŠ¡çš„ä¸¤ä¸ªç‰ˆæœ¬ï¼šoversample å’Œ no-oversample
            main_logger.info(f"ğŸ† å¼€å§‹è®­ç»ƒä¸¤ä¸ªç‰ˆæœ¬: {task_name[:30]}...")
            
            # ç‰ˆæœ¬1: ä½¿ç”¨oversample
            main_logger.info(f"ğŸ’¹ ç‰ˆæœ¬1: ä½¿ç”¨oversample")
            task_auc_oversample, task_output_dir_oversample = sft_lora_single_task(
                args, task_name, task_train_recs, task_val_recs, gnn, vq, tok, base, 
                oversample=True, version_suffix="_oversample"
            )
            
            # ç‰ˆæœ¬2: ä¸ä½¿ç”¨oversample
            main_logger.info(f"ğŸ’· ç‰ˆæœ¬2: ä¸ä½¿ç”¨oversample")
            task_auc_no_oversample, task_output_dir_no_oversample = sft_lora_single_task(
                args, task_name, task_train_recs, task_val_recs, gnn, vq, tok, base, 
                oversample=False, version_suffix="_no_oversample"
            )
            
            # é€‰æ‹©æœ€å¥½çš„ç»“æœ
            if math.isnan(task_auc_oversample) and math.isnan(task_auc_no_oversample):
                best_auc = float("nan")
                best_version = "both_failed"
            elif math.isnan(task_auc_oversample):
                best_auc = task_auc_no_oversample
                best_version = "no_oversample"
            elif math.isnan(task_auc_no_oversample):
                best_auc = task_auc_oversample
                best_version = "oversample"
            else:
                if task_auc_oversample >= task_auc_no_oversample:
                    best_auc = task_auc_oversample
                    best_version = "oversample"
                else:
                    best_auc = task_auc_no_oversample
                    best_version = "no_oversample"
            
            task_results[task_name] = best_auc
            main_logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task_name[:30]}...")
            main_logger.info(f"ğŸ“Š oversample AUC={task_auc_oversample:.4f}, no_oversample AUC={task_auc_no_oversample:.4f}")
            main_logger.info(f"ğŸ† æœ€ä½³ç‰ˆæœ¬: {best_version}, æœ€ç»ˆAUC={best_auc:.4f}")
            
        except Exception as e:
            main_logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {task_name[:30]}... Error: {str(e)}")
            import traceback
            main_logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            failed_tasks.append(task_name)
            # å¼‚å¸¸åä¹Ÿè¦æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                main_logger.info("ğŸ§¹ å¼‚å¸¸åæ¸…ç†æ˜¾å­˜cache")
            continue
    
    # --- æ±‡æ€»ç»“æœ ---
    main_logger.info(f"\nğŸŠ æ‰€æœ‰ä»»åŠ¡è®­ç»ƒå®Œæˆï¼")
    main_logger.info(f"æˆåŠŸ: {len(task_results)} ä¸ªä»»åŠ¡")
    main_logger.info(f"å¤±è´¥: {len(failed_tasks)} ä¸ªä»»åŠ¡")
    
    if task_results:
        valid_aucs = [auc for auc in task_results.values() if not math.isnan(auc)]
        macro_auc = np.mean(valid_aucs) if valid_aucs else float("nan")
        
        main_logger.info(f"\nğŸ“Š ç»“æœæ±‡æ€»:")
        main_logger.info(f"Macro-AUC: {macro_auc:.4f} (over {len(valid_aucs)} valid tasks)")
        
        # è¯¦ç»†ç»“æœ
        main_logger.info("\nğŸ“‹ å„ä»»åŠ¡è¯¦ç»†ç»“æœ:")
        for task_name, auc in sorted(task_results.items(), key=lambda x: x[1], reverse=True):
            short_name = task_name[:50] + "..." if len(task_name) > 50 else task_name
            main_logger.info(f"  {short_name}: AUC={auc:.4f}")
        
        if failed_tasks:
            main_logger.info("\nâŒ å¤±è´¥ä»»åŠ¡:")
            for task in failed_tasks:
                short_name = task[:50] + "..." if len(task) > 50 else task
                main_logger.info(f"  {short_name}")
        
        return macro_auc
    else:
        main_logger.error("âŒ æ²¡æœ‰ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        return float("nan")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI   
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    pa = argparse.ArgumentParser()
    pa.add_argument("--dataset_root", default="")
    pa.add_argument("--tasks", default="hiv")
    pa.add_argument("--stage1_gnn_vq_checkpoint_path", default="")
    pa.add_argument("--stage2_model_path", default="")
    pa.add_argument("--base_tokenizer_path", default="")
    pa.add_argument("--base_llm_model_path", default="")
    pa.add_argument("--vq_codebook_size", type=int, default=512)
    pa.add_argument("--output_dir", default="./prop_out_single_task")
    pa.add_argument("--oversample", type=int, default=1, help="1: ä»»åŠ¡å†…åšminority oversamplingï¼ˆæ¨èå•ä»»åŠ¡ï¼‰; 0: ä¸åšè¿‡é‡‡æ ·")
    pa.add_argument("--use_scaffold_split", type=int, default=1, help="1: ä½¿ç”¨scaffold splitè¿›è¡Œæ•°æ®åˆ†å‰²ï¼ˆé»˜è®¤ï¼‰; 0: ä½¿ç”¨éšæœºåˆ†å‰²")
    pa.add_argument("--balanced_scaffold", type=int, default=1, help="1: éšæœºscaffold splitï¼ˆæ›´å¹³è¡¡ï¼‰; 0: ç¡®å®šæ€§scaffold splitï¼ˆå¯èƒ½ä¸å¹³è¡¡ä½†å¯é‡ç°ï¼‰")
    
    # Task-level oversampling
    pa.add_argument("--task_level_oversample", type=int, default=1, help="1: å¯ç”¨ä»»åŠ¡çº§åˆ«çš„ä¸å¹³è¡¡å¤„ç†ï¼ˆæ¨èï¼‰; 0: ç¦ç”¨")
    pa.add_argument("--min_samples_per_class", type=int, default=10, help="æ¯ä¸ªä»»åŠ¡ä¸­æ¯ä¸ªç±»åˆ«çš„æœ€å°æ ·æœ¬æ•°")
    pa.add_argument("--balance_threshold", type=float, default=0.4, help="ç±»åˆ«å¹³è¡¡é˜ˆå€¼ï¼Œæ­£ä¾‹æ¯”ä¾‹ä½äºæ­¤å€¼æˆ–é«˜äº(1-æ­¤å€¼)æ—¶è¿›è¡Œoversample")

    # training
    pa.add_argument("--num_epochs", type=int, default=10)
    pa.add_argument("--batch_size", type=int, default=3)
    pa.add_argument("--eval_batch_size", type=int, default=4)
    pa.add_argument("--gradient_accumulation_steps", type=int, default=8)
    pa.add_argument("--learning_rate", type=float, default=2e-5)
    pa.add_argument("--weight_decay", type=float, default=1e-2)
    pa.add_argument("--warmup_ratio", type=float, default=0.1)
    pa.add_argument("--max_grad_norm", type=float, default=1.0)
    pa.add_argument("--max_prompt_len", type=int, default=2048)
    pa.add_argument("--max_response_len", type=int, default=16)

    # LoRA
    pa.add_argument("--lora_r", type=int, default=8)
    pa.add_argument("--lora_alpha", type=int, default=32)
    pa.add_argument("--lora_dropout", type=float, default=0.05)
    pa.add_argument("--lora_target_modules", default="q_proj,v_proj,k_proj,o_proj")

    # misc
    pa.add_argument("--device", default="cuda")
    pa.add_argument("--seed", type=int, default=42)
    pa.add_argument("--num_workers", type=int, default=0)
    pa.add_argument("--eval_every_epochs", type=int, default=1)

    # generation (eval)
    pa.add_argument("--num_beams_eval", type=int, default=1)
    pa.add_argument("--do_sample_eval", action="store_true")
    pa.add_argument("--max_seq_length", type=int, default=1024)

    pa.add_argument("--multi_seed", default="12,0,1,2,42", help="é€—å·åˆ†éš”çš„seedåˆ—è¡¨ï¼Œæ¯”å¦‚ '42,123,234,345,456'ã€‚ä¼šè·‘å®Œæ‰€æœ‰seedå¹¶è¾“å‡º meanÂ±std")

    args = pa.parse_args()

    if args.multi_seed:
        seed_list = [int(x) for x in args.multi_seed.split(",") if x.strip()]
        aurocs = []
        task_str = "_".join([t.strip() for t in args.tasks.split(",")])
        for seed in seed_list:
            print(f"\n=== [Seed {seed}] Training... ===\n")
            # è®¾ç½®seedå’Œè¾“å‡ºç›®å½•
            args.seed = seed
            args.output_dir = os.path.join(task_str, f"seed_{seed}")

            # å¦‚æœè¾“å‡ºç›®å½•å­˜åœ¨ï¼Œå»ºè®®æ¸…ç©º
            if os.path.exists(args.output_dir):
                shutil.rmtree(args.output_dir)

            # è·‘ä¸€æ¬¡ä¸»æµç¨‹
            macro_auc = sft_lora_all_tasks(args)
            print(f"[Seed {seed}] Final Macro-AUC: {macro_auc:.4f}")
            aurocs.append(macro_auc)

        # ç»Ÿè®¡ç»“æœ
        aurocs_arr = np.array([x for x in aurocs if not math.isnan(x)])
        print("\n" + "="*60)
        print("ğŸ¯ Multi-Seed Training Summary")
        print("="*60)
        print(f"ğŸ“Š Seeds: {seed_list}")
        print(f"ğŸ“Š Valid Results: {len(aurocs_arr)}/{len(seed_list)}")
        print(f"ğŸ“‹ All AUC: {[f'{v:.4f}' for v in aurocs]}")
        
        if len(aurocs_arr) > 0:
            mean_auc = np.mean(aurocs_arr)
            std_auc = np.std(aurocs_arr, ddof=1) if len(aurocs_arr) > 1 else 0.0
            max_auc = np.max(aurocs_arr)
            min_auc = np.min(aurocs_arr)
            
            print(f"\nğŸ† Final Result: {mean_auc:.4f} Â± {std_auc:.4f}")
            print(f"ğŸ“ˆ Range: [{min_auc:.4f}, {max_auc:.4f}]")
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªseedï¼Œç‰¹åˆ«æç¤º
            if len(aurocs_arr) == 1:
                print("âš ï¸  æ³¨æ„: åªæœ‰1ä¸ªæœ‰æ•ˆç»“æœï¼Œæ— æ³•è®¡ç®—æ ‡å‡†å·®")
        else:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœï¼")
        
        print("="*60)
    else:
        # å•ä¸ªseedè¿è¡Œ
        sft_lora_all_tasks(args)